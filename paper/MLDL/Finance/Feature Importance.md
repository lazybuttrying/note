ì‹ ê²½ë§ì˜ ì¶œë ¥ê°’ $\hat{y}$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Â ê° ì…ë ¥ê°’ $x_i$ ê°€ $\hat{y}$ì— **ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ëŠ”ì§€**ë¥¼ ê³„ì‚°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

## ğŸ”¢ í•µì‹¬ ìˆ˜ì‹: ì…ë ¥ê°’ Ã— ë¯¼ê°ë„(ë¯¸ë¶„)

$$

\text{importance}_i = x_i \cdot \frac{\partial \hat{y}}{\partial x_i}


$$

$$

\boxed{\text{ì¤‘ìš”ë„} = \text{ì…ë ¥ê°’} \times \text{ì¶œë ¥ì˜ ë³€í™”ìœ¨ (ë¯¸ë¶„)}}


$$

- $x_i$ : ì…ë ¥ ë²¡í„°ì˜ ië²ˆì§¸ ê°’
- $\frac{\partial \hat{y}}{\partial x_i}$ : ì¶œë ¥ê°’ì´ $x_i$ì˜ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¯¸ë¶„ê°’
- ì „ì²´ ì‹ì€ **ì…ë ¥ê°’ì˜ í¬ê¸°**ì™€ **ì¶œë ¥ ë³€í™”ìœ¨**ì„ ê³±í•œ ê²ƒ â†’ ì¤‘ìš”ë„

## ğŸ“Œ ì™œ ì´ê±¸ ì“°ë‚˜ìš”?

##### ì˜ˆì‹œ 1: ì…ë ¥ì€ í¬ì§€ë§Œ ì˜í–¥ ì—†ìŒ

- $x_i = 100$
- $\frac{\partial \hat{y}}{\partial x_i} = 0$
  â†’ ì¤‘ìš”ë„ = $100 \cdot 0 = 0$

##### ì˜ˆì‹œ 2: ì…ë ¥ì€ ì‘ì§€ë§Œ ë¯¼ê°í•¨

- $x_i = 0.1$
- $\cfrac{\partial \hat{y}}{\partial x_i} = 50$
  â†’ ì¤‘ìš”ë„ = $0.1 \cdot 50 = 5$

##### ì˜ˆì‹œ 3: ì…ë ¥ë„ í¬ê³  ë¯¼ê°í•¨

- $x_i = 2.0$
- $\cfrac{\partial \hat{y}}{\partial x_i} = 3.0$
  â†’ ì¤‘ìš”ë„ = $2.0 \cdot 3.0 = 6.0$

## ğŸ¯ biasëŠ” ë¬´ì‹œ

#### ì´ìœ  1: biasëŠ” ìƒìˆ˜ì…ë‹ˆë‹¤

$$\hat{y} = w_1 x_1 + w_2 x_2 + b$$

- ì´ë•Œ $b$ëŠ” **ëª¨ë“  ì…ë ¥ê³¼ ë¬´ê´€í•˜ê²Œ** ì¼ì •í•œ ì˜í–¥ì„ ì¤ë‹ˆë‹¤.
- ì¦‰, íŠ¹ì • featureê°€ ë” ì¤‘ìš”í•˜ë‹¤ê³  ë§í•´ì£¼ëŠ” ì§€í‘œê°€ ì•„ë‹™ë‹ˆë‹¤.

#### ì´ìœ  2: ë¯¸ë¶„í•˜ë©´ biasëŠ” ì‚¬ë¼ì§‘ë‹ˆë‹¤

- ìœ„ ì‹ì—ì„œ ë¯¸ë¶„í•´ë³´ë©´, Â $b$ëŠ” ìƒìˆ˜ì´ë¯€ë¡œ ë¯¸ë¶„í•˜ë©´ 0ì´ ë˜ì–´, ì¤‘ìš”ë„ ê³„ì‚°ì— ì˜í–¥ ì—†ìŠµë‹ˆë‹¤.
  $$
  \frac{\partial \hat{y}}{\partial x_1} = w_1, \quad \frac{\partial \hat{y}}{\partial x_2} = w_2
  $$

---

# ğŸ“ MLP Feature Importance â€” Matrix-based Proof

## ğŸ—ï¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (2ì¸µ MLP ì˜ˆì‹œ)

$$

\begin{align*}

\mathbf{h}_1 &= \phi_1(\mathbf{W}_1 \mathbf{x}) \\

\mathbf{h}_2 &= \phi_2(\mathbf{W}_2 \mathbf{h}_1) \\

\hat{y} &= \mathbf{W}_3 \mathbf{h}_2

\end{align*}


$$

ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ 1 â†’ ì€ë‹‰ì¸µ 2 â†’ ì¶œë ¥ì¸µ

- $\mathbf{W}_1 \in \mathbb{R}^{d_1 \times n}$
- $\mathbf{W}_2 \in \mathbb{R}^{d_2 \times d_1}$
- $\mathbf{W}_3 \in \mathbb{R}^{1 \times d_2}$
- bias í•­ì€ ë¬´ì‹œ

## âš™ï¸ ëª©í‘œ: $\frac{\partial \hat{y}}{\partial \mathbf{x}}$

ì²´ì¸ ë£°(Chain Rule)ì— ì˜í•´:

$$
\frac{\partial \hat{y}}{\partial \mathbf{x}} =

\frac{\partial \hat{y}}{\partial \mathbf{h}_2} \cdot

\frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1} \cdot

\frac{\partial \mathbf{h}_1}{\partial \mathbf{x}}
$$

## ğŸ”¢ Derivatives of each Layer

1. Output Layer: $\hat{y} = \mathbf{W}_3 \mathbf{h}_2 \Rightarrow\cfrac{\partial \hat{y}}{\partial \mathbf{h}_2} = \mathbf{W}_3$
2. Second Hidden Layer: $\cfrac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1} =\text{diag}(\phi_2'(\mathbf{z}_2)) \cdot \mathbf{W}_2$
   1. $\mathbf{h}_2 = \phi_2(\mathbf{W}_2 \mathbf{h}_1)$
   2. Let $\mathbf{z}_2 = \mathbf{W}_2 \mathbf{h}_1$, then $\cfrac{\partial \phi_2(\mathbf{z}_2)}{\partial \mathbf{z}_2} = \text{diag} (\phi_2'(\mathbf{z}_2))$
3. First Hidden Layer: $\mathbf{h}_1 = \phi_1(\mathbf{W}_1 \mathbf{x}) \Rightarrow \cfrac{\partial \mathbf{h}_1}{\partial \mathbf{x}} = \text{diag}(\phi_1'(\mathbf{z}_1)) \cdot \mathbf{W}_1$

$$

\therefore \cfrac{\partial \hat{y}}{\partial \mathbf{x}} =

\mathbf{W}_3 \cdot \text{diag}(\phi_2') \cdot \mathbf{W}_2 \cdot \text{diag}(\phi_1') \cdot \mathbf{W}_1


$$

## ğŸ¯ Feature-wise importance

$$

\text{importance}_i = x_i \cdot \frac{\partial \hat{y}}{\partial x_i}


$$

ë²¡í„° í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ ($\odot$ëŠ” element-wise ê³±):

$$

\text{importance} = \mathbf{x} \odot \left( \frac{\partial \hat{y}}{\partial \mathbf{x}} \right)


$$
